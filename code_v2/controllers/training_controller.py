import torch
import os
import cv2
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# è§†å›¾å±‚å¯¼å…¥
from views.cli_view import CLIView

# æ§åˆ¶å™¨å±‚å¯¼å…¥
from controllers.denoise_controller import DenoiseController
from controllers.batch_controller import BatchController
from controllers.sharpening_controller import SharpeningController

# å·¥å…·å‡½æ•°å¯¼å…¥
from utils.image_utils import get_model_path

# å¦‚æœéœ€è¦ç›´æ¥ä½¿ç”¨æ¨¡å‹ç±»
from models.denoiser_models import ImprovedDnCNN
from models.traditional_denoiser import TraditionalDenoiser, AdvancedDenoiser
from models.image_sharpener import ImageSharpener
from models.trainer_model import EarlyStopping, AdvancedDenoisingDataset, ModelTrainer

# å¦‚æœéœ€è¦è®¾å¤‡å·¥å…·
from utils.device_utils import setup_device, check_pytorch_cuda_support

from config import NUM_LAYERS

class TrainingController:
    """è®­ç»ƒæ§åˆ¶å™¨ - æ·»åŠ æŸå¤±æ›²çº¿æ˜¾ç¤º"""
    
    def __init__(self):
        self.model = None
        self.trainer = None
        self.device = setup_device()
        self.train_losses = []
        self.val_losses = []
    
    def setup_environment(self):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        print("ğŸš€ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
        check_pytorch_cuda_support()
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def get_training_parameters(self):
        """è·å–è®­ç»ƒå‚æ•°"""
        print("\nğŸ“‹ è®­ç»ƒå‚æ•°è®¾ç½®:")
        
        params = {
            'epochs': 100,
            'batch_size': 16,
            'learning_rate': 0.001,
            'patience': 10,
            'image_size': (256, 256),
            'max_samples': 5000
        }
        
        try:
            params['epochs'] = int(input(f"è®­ç»ƒè½®æ•° (é»˜è®¤ {params['epochs']}): ") or params['epochs'])
            params['batch_size'] = int(input(f"æ‰¹æ¬¡å¤§å° (é»˜è®¤ {params['batch_size']}): ") or params['batch_size'])
            params['learning_rate'] = float(input(f"å­¦ä¹ ç‡ (é»˜è®¤ {params['learning_rate']}): ") or params['learning_rate'])
            params['patience'] = int(input(f"æ—©åœè€å¿ƒå€¼ (é»˜è®¤ {params['patience']}): ") or params['patience'])
            
            size_input = input(f"å›¾åƒå°ºå¯¸ (é»˜è®¤ {params['image_size'][0]}x{params['image_size'][1]}): ") or f"{params['image_size'][0]}x{params['image_size'][1]}"
            if 'x' in size_input:
                w, h = map(int, size_input.split('x'))
                params['image_size'] = (h, w)  # OpenCV ä½¿ç”¨ (height, width)
            
            params['max_samples'] = int(input(f"æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤ {params['max_samples']}): ") or params['max_samples'])
            
        except ValueError as e:
            print(f"å‚æ•°è¾“å…¥é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        
        print("\nâœ… è®­ç»ƒå‚æ•°:")
        for key, value in params.items():
            print(f"  {key}: {value}")
        
        return params
    
    def prepare_datasets(self, image_folder, image_size, max_samples):
        """å‡†å¤‡æ•°æ®é›†"""
        print(f"\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")
        print(f"å›¾åƒæ–‡ä»¶å¤¹: {image_folder}")
        print(f"å›¾åƒå°ºå¯¸: {image_size}")
        print(f"æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = AdvancedDenoisingDataset(
            image_folder=image_folder,
            target_size=image_size,
            max_samples=max_samples
        )
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=16, shuffle=True, num_workers=0
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=16, shuffle=False, num_workers=0
        )
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        return train_loader, val_loader
    
    def initialize_model(self, num_layers=17):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print(f"\nåˆå§‹åŒ–ImprovedDnCNNæ¨¡å‹ ({num_layers}å±‚)...")
        self.model = ImprovedDnCNN(channels=3, num_layers=num_layers, num_features=64)
        self.trainer = ModelTrainer(self.model, "trained_models")
        return self.model
    
    def start_training(self, train_loader, val_loader, params):
        """å¼€å§‹è®­ç»ƒ"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        print(f"æ€»è½®æ•°: {params['epochs']}")
        print(f"æ‰¹æ¬¡å¤§å°: {params['batch_size']}")
        print(f"å­¦ä¹ ç‡: {params['learning_rate']}")
        
        # å¼€å§‹è®­ç»ƒå¹¶è·å–æŸå¤±å†å²
        self.train_losses, self.val_losses, best_val_loss = self.trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=params['epochs'],
            lr=params['learning_rate'],
            patience=params['patience']
        )
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': best_val_loss,
            'epochs_trained': len(self.train_losses)
        }
    
    def display_training_results(self, results):
        """æ˜¾ç¤ºè®­ç»ƒç»“æœå’ŒæŸå¤±æ›²çº¿"""
        print(f"\nğŸ“ˆ è®­ç»ƒå®Œæˆ!")
        print(f"è®­ç»ƒè½®æ•°: {results['epochs_trained']}")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.6f}")
        
        # æ˜¾ç¤ºæŸå¤±æ›²çº¿
        self._plot_loss_curves(results)
        
        # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
        self._display_training_stats(results)
    
    def _plot_loss_curves(self, results):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        try:
            plt.figure(figsize=(12, 8))
            
            # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±
            epochs = range(1, len(results['train_losses']) + 1)
            
            plt.subplot(2, 1, 1)
            plt.plot(epochs, results['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
            plt.plot(epochs, results['val_losses'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
            plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('æŸå¤± (MSE)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ç»˜åˆ¶å¯¹æ•°å°ºåº¦æŸå¤±
            plt.subplot(2, 1, 2)
            plt.semilogy(epochs, results['train_losses'], 'b-', label='train loss', linewidth=2, alpha=0.7)
            plt.semilogy(epochs, results['val_losses'], 'r-', label='valid loss', linewidth=2, alpha=0.7)
            plt.title('å¯¹æ•°å°ºåº¦æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('æŸå¤± (log MSE)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
            # ä¿å­˜æŸå¤±æ›²çº¿
            self._save_loss_plot(results)
            
        except Exception as e:
            print(f"ç»˜åˆ¶æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")
    
    def _plot_loss_curves(self, results):
        """Plot loss curves"""
        try:
            plt.figure(figsize=(12, 8))
            
            # Plot training and validation losses
            epochs = range(1, len(results['train_losses']) + 1)
            
            plt.subplot(2, 1, 1)
            plt.plot(epochs, results['train_losses'], 'b-', label='Training Loss', linewidth=2)
            plt.plot(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2)
            plt.title('Training and Validation Loss Curves', fontsize=14, fontweight='bold')
            plt.xlabel('Epochs')
            plt.ylabel('Loss (MSE)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Plot log-scale losses
            plt.subplot(2, 1, 2)
            plt.semilogy(epochs, results['train_losses'], 'b-', label='Training Loss', linewidth=2, alpha=0.7)
            plt.semilogy(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2, alpha=0.7)
            plt.title('Log-Scale Loss Curves', fontsize=14, fontweight='bold')
            plt.xlabel('Epochs')
            plt.ylabel('Loss (log MSE)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
            # Save loss plot
            self._save_loss_plot(results)
        
        except Exception as e:
            print(f"Error plotting loss curves: {e}")
    
    def _display_training_stats(self, results):
        """æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        train_losses = results['train_losses']
        val_losses = results['val_losses']
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
        
        # è®¡ç®—æ”¹è¿›ç¨‹åº¦
        initial_train_loss = train_losses[0] if train_losses else 0
        initial_val_loss = val_losses[0] if val_losses else 0
        
        if initial_train_loss > 0:
            train_improvement = (initial_train_loss - train_losses[-1]) / initial_train_loss * 100
            print(f"è®­ç»ƒæŸå¤±æ”¹è¿›: {train_improvement:.1f}%")
        
        if initial_val_loss > 0:
            val_improvement = (initial_val_loss - val_losses[-1]) / initial_val_loss * 100
            print(f"éªŒè¯æŸå¤±æ”¹è¿›: {val_improvement:.1f}%")
        
        # æ˜¾ç¤ºæŸå¤±èŒƒå›´
        if train_losses:
            print(f"è®­ç»ƒæŸå¤±èŒƒå›´: {min(train_losses):.6f} - {max(train_losses):.6f}")
        if val_losses:
            print(f"éªŒè¯æŸå¤±èŒƒå›´: {min(val_losses):.6f} - {max(val_losses):.6f}")

def model_training():
    """æ¨¡å‹è®­ç»ƒæ¨¡å¼"""
    try:
        controller = TrainingController()
        
        # è®¾ç½®ç¯å¢ƒ
        controller.setup_environment()
        
        # è·å–è®­ç»ƒå‚æ•°
        params = controller.get_training_parameters()
        
        # è·å–å›¾åƒæ–‡ä»¶å¤¹
        image_folder = input("\nè¯·è¾“å…¥åŒ…å«è®­ç»ƒå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip().strip('"\'')
        if not os.path.exists(image_folder):
            print(f"é”™è¯¯ï¼šæ–‡ä»¶å¤¹ '{image_folder}' ä¸å­˜åœ¨ï¼")
            return
        
        # å‡†å¤‡æ•°æ®
        train_loader, val_loader = controller.prepare_datasets(
            image_folder, 
            params['image_size'], 
            params['max_samples']
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        controller.initialize_model(NUM_LAYERS)
        
        # å¼€å§‹è®­ç»ƒ
        results = controller.start_training(train_loader, val_loader, params)
        
        # æ˜¾ç¤ºç»“æœå’ŒæŸå¤±æ›²çº¿
        controller.display_training_results(results)
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    def _save_loss_plot(self, results, save_dir="training_results"):
        """
        ä¿å­˜æŸå¤±æ›²çº¿å›¾åˆ°æ–‡ä»¶
        
        å‚æ•°:
            results: åŒ…å«è®­ç»ƒç»“æœçš„å­—å…¸
            save_dir: ä¿å­˜ç›®å½•
        """
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(save_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"loss_curves_{timestamp}.png"
            filepath = os.path.join(save_dir, filename)
            
            # åˆ›å»ºå›¾è¡¨
            plt.figure(figsize=(15, 10))
            
            # 1. è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
            plt.subplot(2, 2, 1)
            epochs = range(1, len(results['train_losses']) + 1)
            
            plt.plot(epochs, results['train_losses'], 'b-', linewidth=2, label='Training Loss', alpha=0.8)
            plt.plot(epochs, results['val_losses'], 'r-', linewidth=2, label='Validation Loss', alpha=0.8)
            plt.title('Training and Validation Loss', fontsize=12, fontweight='bold')
            plt.xlabel('Epochs')
            plt.ylabel('MSE Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # 2. å¯¹æ•°å°ºåº¦æŸå¤±æ›²çº¿
            plt.subplot(2, 2, 2)
            plt.semilogy(epochs, results['train_losses'], 'b-', linewidth=2, label='Training Loss', alpha=0.8)
            plt.semilogy(epochs, results['val_losses'], 'r-', linewidth=2, label='Validation Loss', alpha=0.8)
            plt.title('Log-Scale Loss Curves', fontsize=12, fontweight='bold')
            plt.xlabel('Epochs')
            plt.ylabel('Log MSE Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # 3. æŸå¤±æ”¹è¿›ç‡
            plt.subplot(2, 2, 3)
            if len(results['train_losses']) > 1:
                train_improvements = []
                val_improvements = []
                
                for i in range(1, len(results['train_losses'])):
                    train_imp = (results['train_losses'][i-1] - results['train_losses'][i]) / results['train_losses'][i-1] * 100
                    val_imp = (results['val_losses'][i-1] - results['val_losses'][i]) / results['val_losses'][i-1] * 100
                    train_improvements.append(train_imp)
                    val_improvements.append(val_imp)
                
                plt.plot(range(1, len(train_improvements) + 1), train_improvements, 'g-', 
                        linewidth=2, label='Training Improvement %', alpha=0.8)
                plt.plot(range(1, len(val_improvements) + 1), val_improvements, 'm-', 
                        linewidth=2, label='Validation Improvement %', alpha=0.8)
                plt.title('Epoch-to-Epoch Improvement Rate', fontsize=12, fontweight='bold')
                plt.xlabel('Epoch Transition')
                plt.ylabel('Improvement Percentage (%)')
                plt.legend()
                plt.grid(True, alpha=0.3)
            
            # 4. æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
            plt.subplot(2, 2, 4)
            # æ¸…ç©ºè¿™ä¸ªå­å›¾ï¼Œç”¨äºæ˜¾ç¤ºæ–‡æœ¬ä¿¡æ¯
            plt.axis('off')
            
            # å‡†å¤‡ç»Ÿè®¡æ–‡æœ¬
            stats_text = [
                "TRAINING STATISTICS",
                "=" * 20,
                f"Total Epochs: {len(results['train_losses'])}",
                f"Best Val Loss: {results['best_val_loss']:.6f}",
                f"Final Train Loss: {results['train_losses'][-1]:.6f}",
                f"Final Val Loss: {results['val_losses'][-1]:.6f}",
                "",
                "IMPROVEMENTS",
                "=" * 20
            ]
            
            # è®¡ç®—æ€»ä½“æ”¹è¿›
            if len(results['train_losses']) > 1:
                total_train_imp = (results['train_losses'][0] - results['train_losses'][-1]) / results['train_losses'][0] * 100
                total_val_imp = (results['val_losses'][0] - results['val_losses'][-1]) / results['val_losses'][0] * 100
                stats_text.extend([
                    f"Train Improvement: {total_train_imp:.1f}%",
                    f"Val Improvement: {total_val_imp:.1f}%"
                ])
            
            # æ·»åŠ è®­ç»ƒå‚æ•°ä¿¡æ¯
            stats_text.extend([
                "",
                "TRAINING INFO",
                "=" * 20,
                f"Timestamp: {timestamp}",
                f"Device: {self.device}",
                f"Model: ImprovedDnCNN-{NUM_LAYERS}L"
            ])
            
            # æ˜¾ç¤ºæ–‡æœ¬
            plt.text(0.1, 0.95, '\n'.join(stats_text), transform=plt.gca().transAxes,
                    fontfamily='monospace', fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.7))
            
            # è®¾ç½®æ€»æ ‡é¢˜
            plt.suptitle('DnCNN Training Analysis Report', fontsize=16, fontweight='bold', y=0.98)
            
            # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
            plt.tight_layout()
            plt.subplots_adjust(top=0.93)
            plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print(f"âœ… æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³: {filepath}")
            
            # åŒæ—¶ä¿å­˜æŸå¤±æ•°æ®ä¸ºCSVæ–‡ä»¶
            self._save_loss_data(results, save_dir, timestamp)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

    def _save_loss_data(self, results, save_dir, timestamp):
        """
        ä¿å­˜æŸå¤±æ•°æ®ä¸ºCSVæ–‡ä»¶
        
        å‚æ•°:
            results: è®­ç»ƒç»“æœ
            save_dir: ä¿å­˜ç›®å½•
            timestamp: æ—¶é—´æˆ³
        """
        try:
            csv_filename = f"loss_data_{timestamp}.csv"
            csv_filepath = os.path.join(save_dir, csv_filename)
            
            # å‡†å¤‡æ•°æ®
            epochs = range(1, len(results['train_losses']) + 1)
            
            # åˆ›å»ºDataFrame
            import pandas as pd
            loss_data = {
                'epoch': list(epochs),
                'train_loss': results['train_losses'],
                'val_loss': results['val_losses']
            }
            
            # è®¡ç®—æ”¹è¿›ç‡
            if len(results['train_losses']) > 1:
                train_improvements = [0]  # ç¬¬ä¸€è½®æ²¡æœ‰æ”¹è¿›
                val_improvements = [0]
                
                for i in range(1, len(results['train_losses'])):
                    train_imp = (results['train_losses'][i-1] - results['train_losses'][i]) / results['train_losses'][i-1] * 100
                    val_imp = (results['val_losses'][i-1] - results['val_losses'][i]) / results['val_losses'][i-1] * 100
                    train_improvements.append(train_imp)
                    val_improvements.append(val_imp)
                
                loss_data['train_improvement_%'] = train_improvements
                loss_data['val_improvement_%'] = val_improvements
            
            df = pd.DataFrame(loss_data)
            df.to_csv(csv_filepath, index=False, encoding='utf-8')
            
            print(f"âœ… æŸå¤±æ•°æ®å·²ä¿å­˜è‡³: {csv_filepath}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŸå¤±æ•°æ®æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    model_training()