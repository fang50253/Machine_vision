import torch
import sys
import cv2
import platform

def setup_device():
    """è®¾ç½®è®­ç»ƒè®¾å¤‡ - è·¨å¹³å°å…¼å®¹ç‰ˆæœ¬"""
    print("æ­£åœ¨æ£€æµ‹å¯ç”¨è®¾å¤‡...")
    
    system = platform.system()
    print(f"å½“å‰æ“ä½œç³»ç»Ÿ: {system}")
    
    if system == "Darwin":  # macOS
        return _setup_macos_device()
    elif system == "Windows":  # Windows
        return _setup_windows_device()
    else:  # Linux æˆ–å…¶ä»–ç³»ç»Ÿ
        return _setup_linux_device()

def _setup_macos_device():
    """è®¾ç½® macOS è®¾å¤‡"""
    print("æ£€æµ‹åˆ° macOS ç³»ç»Ÿ")
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        # æ£€æŸ¥ MPS å¯ç”¨æ€§
        if torch.backends.mps.is_built():
            device = torch.device('mps')
            
            try:
                # æµ‹è¯• MPS è®¾å¤‡
                test_tensor = torch.tensor([1.0]).to(device)
                result = test_tensor * 2
                
                print("âœ“ Apple Silicon GPU (MPS) å¯ç”¨")
                print(f"  æ­£åœ¨ä½¿ç”¨ MPS è®¾å¤‡è¿›è¡Œè®­ç»ƒ")
                
                # è®¾ç½® MPS ä¼˜åŒ–
                torch.backends.mps.enabled = True
                
                return device
                
            except Exception as e:
                print(f"âœ— MPS è®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
                print("  å›é€€åˆ° CPU")
                return torch.device('cpu')
        else:
            print("âœ— PyTorch æœªæ„å»º MPS æ”¯æŒ")
            print("  è¯·å®‰è£…æ”¯æŒ MPS çš„ PyTorch ç‰ˆæœ¬")
            return torch.device('cpu')
    else:
        print("âœ— MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
        return torch.device('cpu')

def _setup_windows_device():
    """è®¾ç½® Windows è®¾å¤‡"""
    print("æ£€æµ‹åˆ° Windows ç³»ç»Ÿ")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        device = torch.device('cuda')
        
        gpu_count = torch.cuda.device_count()
        print(f"å‘ç° {gpu_count} ä¸ª NVIDIA GPU è®¾å¤‡:")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            compute_capability = f"{torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}"
            print(f"  GPU {i}: {gpu_name}")
            print(f"    æ˜¾å­˜: {gpu_memory:.1f} GB")
            print(f"    è®¡ç®—èƒ½åŠ›: {compute_capability}")
        
        # é€‰æ‹©æœ€ä½³ GPUï¼ˆé€šå¸¸æ˜¯ 0 å·ï¼‰
        torch.cuda.set_device(0)
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        
        try:
            # æµ‹è¯• GPU è®¡ç®—
            test_tensor = torch.tensor([1.0]).cuda()
            if test_tensor.is_cuda:
                print("âœ“ GPU æµ‹è¯•é€šè¿‡ï¼Œæ­£åœ¨ä½¿ç”¨ GPU è¿›è¡Œè®­ç»ƒ")
                return device
            else:
                print("âœ— GPU æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ° CPU")
                return torch.device('cpu')
        except Exception as e:
            print(f"âœ— GPU æµ‹è¯•å¤±è´¥: {e}")
            return torch.device('cpu')
    else:
        print("âœ— æœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA è®¾å¤‡ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
        return torch.device('cpu')

def _setup_linux_device():
    """è®¾ç½® Linux è®¾å¤‡"""
    print("æ£€æµ‹åˆ° Linux ç³»ç»Ÿ")
    
    if torch.cuda.is_available():
        # ä½¿ç”¨ä¸ Windows ç›¸åŒçš„ CUDA è®¾ç½®
        return _setup_windows_device()
    else:
        print("âœ— æœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA è®¾å¤‡ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
        return torch.device('cpu')

def check_pytorch_device_support():
    """æ£€æŸ¥ PyTorch è®¾å¤‡æ”¯æŒ - è·¨å¹³å°ç‰ˆæœ¬"""
    print("\n" + "="*60)
    print("PyTorch è®¾å¤‡æ”¯æŒè¯Šæ–­")
    print("="*60)
    
    system = platform.system()
    print(f"æ“ä½œç³»ç»Ÿ: {system}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥ CUDA æ”¯æŒ
    print(f"\nCUDA æ”¯æŒ:")
    print(f"  torch.cuda.is_available(): {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  torch.cuda.device_count(): {torch.cuda.device_count()}")
        print(f"  torch.version.cuda: {torch.version.cuda}")
        print(f"  cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    # æ£€æŸ¥ MPS æ”¯æŒ (macOS)
    print(f"\nApple Silicon GPU (MPS) æ”¯æŒ:")
    if hasattr(torch.backends, 'mps'):
        print(f"  torch.backends.mps.is_available(): {torch.backends.mps.is_available()}")
        print(f"  torch.backends.mps.is_built(): {torch.backends.mps.is_built()}")
    else:
        print("  MPS åç«¯ä¸å¯ç”¨")
    
    # è®¾å¤‡æµ‹è¯•
    print(f"\nè®¾å¤‡æµ‹è¯•:")
    devices_to_test = []
    
    if torch.cuda.is_available():
        devices_to_test.append(('CUDA', torch.device('cuda')))
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        devices_to_test.append(('MPS', torch.device('mps')))
    
    devices_to_test.append(('CPU', torch.device('cpu')))
    
    for device_name, device in devices_to_test:
        try:
            x = torch.randn(3, 3).to(device)
            y = torch.randn(3, 3).to(device)
            z = x + y
            print(f"  âœ“ {device_name} è®¡ç®—æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•çŸ©é˜µä¹˜æ³•ï¼ˆæ›´å¤æ‚çš„æ“ä½œï¼‰
            if device_name != 'CPU':  # CPU æ€»æ˜¯èƒ½å·¥ä½œ
                a = torch.randn(100, 100).to(device)
                b = torch.randn(100, 100).to(device)
                c = torch.mm(a, b)
                print(f"  âœ“ {device_name} çŸ©é˜µä¹˜æ³•æµ‹è¯•é€šè¿‡")
                
        except Exception as e:
            print(f"  âœ— {device_name} è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
    
    # æ¨èçš„æœ€ä½³è®¾å¤‡
    best_device = setup_device()
    print(f"\nğŸ¯ æ¨èä½¿ç”¨è®¾å¤‡: {best_device}")
    
    print("="*60)
    return best_device

def get_device_info(device):
    """è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯"""
    info = {
        'type': str(device),
        'system': platform.system(),
        'pytorch_version': torch.__version__
    }
    
    if device.type == 'cuda':
        info['gpu_name'] = torch.cuda.get_device_name(device)
        info['gpu_memory_gb'] = torch.cuda.get_device_properties(device).total_memory / 1024**3
        info['cuda_version'] = torch.version.cuda
    elif device.type == 'mps':
        info['device_name'] = 'Apple Silicon GPU'
        info['backend'] = 'MPS (Metal Performance Shaders)'
    
    return info

def print_device_info(device):
    """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
    info = get_device_info(device)
    
    print("\n" + "ğŸ¯ å½“å‰è®­ç»ƒè®¾å¤‡ä¿¡æ¯:")
    print(f"  è®¾å¤‡ç±»å‹: {info['type']}")
    print(f"  æ“ä½œç³»ç»Ÿ: {info['system']}")
    print(f"  PyTorchç‰ˆæœ¬: {info['pytorch_version']}")
    
    if device.type == 'cuda':
        print(f"  GPUåç§°: {info['gpu_name']}")
        print(f"  æ˜¾å­˜: {info['gpu_memory_gb']:.1f} GB")
        print(f"  CUDAç‰ˆæœ¬: {info['cuda_version']}")
    elif device.type == 'mps':
        print(f"  è®¾å¤‡: {info['device_name']}")
        print(f"  åç«¯: {info['backend']}")
    else:
        print(f"  ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è¿è¡Œè®¾å¤‡è¯Šæ–­
    device = check_pytorch_device_support()
    
    # æ‰“å°æœ€ç»ˆé€‰æ‹©çš„è®¾å¤‡ä¿¡æ¯
    print_device_info(device)
    
    # ç¤ºä¾‹ï¼šåˆ›å»ºä¸€äº›æµ‹è¯•æ•°æ®
    print(f"\nğŸ§ª è®¾å¤‡æ€§èƒ½æµ‹è¯•:")
    x = torch.randn(1000, 1000).to(device)
    
    import time
    start_time = time.time()
    
    # æ‰§è¡Œä¸€äº›è®¡ç®—å¯†é›†å‹æ“ä½œ
    for _ in range(100):
        x = torch.matmul(x, x) * 0.99
    
    end_time = time.time()
    print(f"  è®¡ç®—è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"  æœ€ç»ˆè®¾å¤‡: {x.device}")
# åœ¨ device_utils.py æ–‡ä»¶æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š

def check_pytorch_cuda_support():
    """
    å…¼å®¹æ€§å‡½æ•° - ä¿æŒæ—§ä»£ç çš„å¯¼å…¥æ­£å¸¸å·¥ä½œ
    æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°åªæ£€æŸ¥ CUDAï¼Œä¸æ£€æŸ¥ MPS
    """
    print("\n" + "="*50)
    print("PyTorch CUDAæ”¯æŒè¯Šæ–­ (å…¼å®¹æ¨¡å¼)")
    print("="*50)
    
    print(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
    print(f"torch.cuda.device_count(): {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        print(f"torch.version.cuda: {torch.version.cuda}")
        print(f"torch.backends.cudnn.version(): {torch.backends.cudnn.version()}")
        
        try:
            x = torch.randn(3, 3).cuda()
            y = torch.randn(3, 3).cuda()
            z = x + y
            print("âœ“ GPUè®¡ç®—æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âœ— GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
    else:
        print("âœ— CUDAä¸å¯ç”¨")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    print("="*50)
    
    # è¿”å›è®¾å¤‡ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
    return setup_device()

# å¯é€‰ï¼šæ·»åŠ å…¶ä»–å…¼å®¹æ€§å‡½æ•°
def get_available_device():
    """å…¼å®¹æ€§å‡½æ•° - æ›¿ä»£æ—§çš„è®¾å¤‡è·å–æ–¹å¼"""
    return setup_device()